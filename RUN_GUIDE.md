# Quick Start Guide - Optimized Models

## üöÄ Run All Notebooks

Execute these commands in PowerShell:

```powershell
# 1. Activate virtual environment
cd "C:\Users\Kaustab das\Desktop\deep_learning_models"
.\venv\Scripts\Activate.ps1

# 2. Launch Jupyter
jupyter notebook
```

## üìã Execution Order

### Step 1: Data Exploration (Optional - for visualization)
- Open `01_data_exploration.ipynb`
- Run All Cells (Kernel ‚Üí Restart & Run All)
- Expected time: ~5 minutes

### Step 2: Data Preprocessing (REQUIRED)
- Open `02_data_preprocessing.ipynb`
- Run All Cells (Kernel ‚Üí Restart & Run All)
- Creates `processed_data/` folder with:
  - `delay_*.npy`
  - `price_*.npy`
  - `passenger_*.npy`
  - `*.pkl` (encoders)
- Expected time: ~3-5 minutes

### Step 3: Delay Prediction (Already trained)
- `03_delay_prediction_basic.ipynb` 
- ‚úÖ Already excellent (99.88% accuracy)
- No need to re-run unless you want

### Step 4: Price Prediction (OPTIMIZED) ‚≠ê
- Open `04_price_prediction_dnn.ipynb`
- **Important**: Restart kernel before running
- Run All Cells
- **Improvements**:
  - Larger model (512‚Üí256‚Üí128‚Üí64)
  - AdamW optimizer
  - Gradient clipping
  - Better regularization
- Expected time: ~15-25 minutes
- Expected improvement: MAPE 60% ‚Üí <40%, R¬≤ 0.33 ‚Üí >0.60

### Step 5: Passenger LSTM (OPTIMIZED) ‚≠ê
- Open `05_passenger_forecasting_lstm.ipynb`
- **Important**: Restart kernel before running
- Run All Cells
- **Improvements**:
  - Bidirectional LSTM
  - Attention mechanism
  - HuberLoss (robust to outliers)
  - OneCycleLR scheduler
  - Layer normalization
- Expected time: ~20-35 minutes
- Expected improvement: MAPE 86% ‚Üí <30%, R¬≤ -0.17 ‚Üí >0.70

## ‚ö° Quick Commands

### In Jupyter Notebook:
- **Restart Kernel**: `Kernel` ‚Üí `Restart Kernel...`
- **Run All**: `Kernel` ‚Üí `Restart & Run All`
- **Run Single Cell**: `Shift + Enter`
- **Stop Execution**: Press `‚èπ` (Stop button) or `Kernel` ‚Üí `Interrupt`

## üìä Expected Results After Optimization

| Model | Before | After (Target) |
|-------|--------|----------------|
| **Price MAE** | ‚Çπ12,314 | < ‚Çπ8,000 |
| **Price MAPE** | 59.9% | < 40% |
| **Price R¬≤** | 0.33 | > 0.60 |
| **LSTM MAE** | 14,393M | < 5,000M |
| **LSTM MAPE** | 86.1% | < 30% |
| **LSTM R¬≤** | -0.17 | > 0.70 |

## üîç Monitoring Training

### Watch for these indicators:

**Good Signs:**
- ‚úÖ Train loss steadily decreasing
- ‚úÖ Val loss decreasing (with small gap from train loss)
- ‚úÖ No NaN values
- ‚úÖ Gradients not exploding

**Warning Signs:**
- ‚ö†Ô∏è Val loss >> Train loss (overfitting)
- ‚ö†Ô∏è Loss = NaN (exploding gradients)
- ‚ö†Ô∏è No improvement after 20 epochs (learning rate too low)

## üêõ Troubleshooting

### Problem: "processed_data folder not found"
**Solution**: Run notebook 02 first

### Problem: "CUDA out of memory"
**Solution**: 
- Reduce batch_size in the notebook
- Close other applications
- Restart kernel

### Problem: "Loss is NaN"
**Solution**:
- Already fixed with gradient clipping
- If still occurs, reduce learning rate by 50%

### Problem: "Training too slow"
**Solution**:
- Reduce `num_epochs` from 100 to 50
- Reduce model size (hidden_dims)
- Use CPU instead (set `device = 'cpu'`)

## üìù Notes

- Both optimized models use **gradient clipping** to prevent exploding gradients
- **Notebook 04** uses AdamW with ReduceLROnPlateau
- **Notebook 05** uses AdamW with OneCycleLR (super-convergence)
- All improvements are already applied to the code
- Just need to **restart kernel and run all cells**

## üìß After Training

Check the final output cells to see:
- Model performance metrics
- Saved model paths
- Comparison with baseline
- Next steps recommendations

Good luck! üéØ
