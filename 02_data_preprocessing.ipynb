{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1ad70d5",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c428e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Utilities\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create output directory\n",
    "OUTPUT_DIR = Path('processed_data')\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e23d8f6",
   "metadata": {},
   "source": [
    "## 2. Load Raw Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f00a5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "\n",
      "‚úÖ Loaded 171,666 delay records\n",
      "‚úÖ Loaded 300,153 pricing records\n",
      "‚úÖ Loaded 7,242 passenger records\n",
      "‚úÖ Loaded 322 airports\n",
      "‚úÖ Loaded 44,393 holiday records\n",
      "‚úÖ Loaded 107,963 weather records\n",
      "‚úÖ Loaded 171,666 delay records\n",
      "‚úÖ Loaded 300,153 pricing records\n",
      "‚úÖ Loaded 7,242 passenger records\n",
      "‚úÖ Loaded 322 airports\n",
      "‚úÖ Loaded 44,393 holiday records\n",
      "‚úÖ Loaded 107,963 weather records\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path('dataset')\n",
    "\n",
    "print(\"Loading datasets...\\n\")\n",
    "\n",
    "df_delays = pd.read_csv(DATA_DIR / 'Airline_Delay_Cause.csv')\n",
    "df_pricing = pd.read_csv(DATA_DIR / 'airlines_flights_data.csv')\n",
    "df_passengers = pd.read_csv(DATA_DIR / 'monthly_passengers.csv')\n",
    "df_airports = pd.read_csv(DATA_DIR / 'airports.csv')\n",
    "df_airlines = pd.read_csv(DATA_DIR / 'airlines.csv')\n",
    "df_holidays = pd.read_csv(DATA_DIR / 'global_holidays.csv')\n",
    "df_weather = pd.read_csv(DATA_DIR / 'GlobalWeatherRepository.csv')\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(df_delays):,} delay records\")\n",
    "print(f\"‚úÖ Loaded {len(df_pricing):,} pricing records\")\n",
    "print(f\"‚úÖ Loaded {len(df_passengers):,} passenger records\")\n",
    "print(f\"‚úÖ Loaded {len(df_airports):,} airports\")\n",
    "print(f\"‚úÖ Loaded {len(df_holidays):,} holiday records\")\n",
    "print(f\"‚úÖ Loaded {len(df_weather):,} weather records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f154e856",
   "metadata": {},
   "source": [
    "## 3. Project 1: Delay Prediction Dataset\n",
    "\n",
    "### 3.1 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08f89e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DELAY PREDICTION - FEATURE ENGINEERING\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Created 29 features\n",
      "‚úÖ Binary delay distribution: {1: 164638, 0: 7028}\n",
      "‚úÖ Multi-class causes: {'carrier': 71816, 'late_aircraft': 52115, 'nas': 39106, 'none': 7028, 'weather': 1560, 'security': 41}\n",
      "\n",
      "‚úÖ Created 29 features\n",
      "‚úÖ Binary delay distribution: {1: 164638, 0: 7028}\n",
      "‚úÖ Multi-class causes: {'carrier': 71816, 'late_aircraft': 52115, 'nas': 39106, 'none': 7028, 'weather': 1560, 'security': 41}\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"DELAY PREDICTION - FEATURE ENGINEERING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create a copy\n",
    "df_delay_model = df_delays.copy()\n",
    "\n",
    "# 1. Binary delay target (15+ minutes)\n",
    "df_delay_model['is_delayed'] = (df_delay_model['arr_del15'] > 0).astype(int)\n",
    "\n",
    "# 2. Delay rate feature\n",
    "df_delay_model['delay_rate'] = (df_delay_model['arr_del15'] / df_delay_model['arr_flights']) * 100\n",
    "df_delay_model['delay_rate'] = df_delay_model['delay_rate'].fillna(0)\n",
    "\n",
    "# 3. Cancellation rate\n",
    "df_delay_model['cancel_rate'] = (df_delay_model['arr_cancelled'] / df_delay_model['arr_flights']) * 100\n",
    "df_delay_model['cancel_rate'] = df_delay_model['cancel_rate'].fillna(0)\n",
    "\n",
    "# 4. Temporal features (cyclic encoding)\n",
    "df_delay_model['month_sin'] = np.sin(2 * np.pi * df_delay_model['month'] / 12)\n",
    "df_delay_model['month_cos'] = np.cos(2 * np.pi * df_delay_model['month'] / 12)\n",
    "\n",
    "# 5. Dominant delay cause (multi-class target)\n",
    "delay_cause_cols = ['carrier_ct', 'weather_ct', 'nas_ct', 'security_ct', 'late_aircraft_ct']\n",
    "df_delay_model['dominant_cause'] = df_delay_model[delay_cause_cols].idxmax(axis=1)\n",
    "df_delay_model['dominant_cause'] = df_delay_model['dominant_cause'].str.replace('_ct', '')\n",
    "\n",
    "# Handle rows with no delays (all zeros)\n",
    "no_delays = df_delay_model[delay_cause_cols].sum(axis=1) == 0\n",
    "df_delay_model.loc[no_delays, 'dominant_cause'] = 'none'\n",
    "\n",
    "# 6. Historical carrier performance (rolling average)\n",
    "df_delay_model = df_delay_model.sort_values(['carrier', 'year', 'month'])\n",
    "df_delay_model['carrier_delay_history'] = df_delay_model.groupby('carrier')['delay_rate'].transform(\n",
    "    lambda x: x.rolling(window=3, min_periods=1).mean()\n",
    ")\n",
    "\n",
    "# 7. Historical airport performance\n",
    "df_delay_model['airport_delay_history'] = df_delay_model.groupby('airport')['delay_rate'].transform(\n",
    "    lambda x: x.rolling(window=3, min_periods=1).mean()\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Created {df_delay_model.shape[1]} features\")\n",
    "print(f\"‚úÖ Binary delay distribution: {df_delay_model['is_delayed'].value_counts().to_dict()}\")\n",
    "print(f\"‚úÖ Multi-class causes: {df_delay_model['dominant_cause'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43632dc3",
   "metadata": {},
   "source": [
    "### 3.2 Handle Missing Values & Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31b31f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Missing Values Before Processing:\n",
      "arr_flights            240\n",
      "arr_del15              443\n",
      "carrier_ct             240\n",
      "weather_ct             240\n",
      "nas_ct                 240\n",
      "security_ct            240\n",
      "late_aircraft_ct       240\n",
      "arr_cancelled          240\n",
      "arr_diverted           240\n",
      "arr_delay              240\n",
      "carrier_delay          240\n",
      "weather_delay          240\n",
      "nas_delay              240\n",
      "security_delay         240\n",
      "late_aircraft_delay    240\n",
      "dtype: int64\n",
      "\n",
      "‚úÖ Encoding complete!\n",
      "  - Carriers: 21 unique\n",
      "  - Airports: 395 unique\n",
      "  - Delay Causes: ['carrier', 'late_aircraft', 'nas', 'none', 'security', 'weather']\n"
     ]
    }
   ],
   "source": [
    "# Check missing values\n",
    "print(\"\\nüîç Missing Values Before Processing:\")\n",
    "missing = df_delay_model.isnull().sum()\n",
    "if missing.sum() > 0:\n",
    "    print(missing[missing > 0])\n",
    "else:\n",
    "    print(\"‚úÖ No missing values!\")\n",
    "\n",
    "# Fill any remaining missing values\n",
    "numeric_cols = df_delay_model.select_dtypes(include=[np.number]).columns\n",
    "df_delay_model[numeric_cols] = df_delay_model[numeric_cols].fillna(0)\n",
    "\n",
    "# Encode categorical features\n",
    "le_carrier = LabelEncoder()\n",
    "le_airport = LabelEncoder()\n",
    "le_cause = LabelEncoder()\n",
    "\n",
    "df_delay_model['carrier_encoded'] = le_carrier.fit_transform(df_delay_model['carrier'])\n",
    "df_delay_model['airport_encoded'] = le_airport.fit_transform(df_delay_model['airport'])\n",
    "df_delay_model['cause_encoded'] = le_cause.fit_transform(df_delay_model['dominant_cause'])\n",
    "\n",
    "# Save encoders\n",
    "with open(OUTPUT_DIR / 'delay_encoders.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'carrier': le_carrier,\n",
    "        'airport': le_airport,\n",
    "        'cause': le_cause\n",
    "    }, f)\n",
    "\n",
    "print(\"\\n‚úÖ Encoding complete!\")\n",
    "print(f\"  - Carriers: {len(le_carrier.classes_)} unique\")\n",
    "print(f\"  - Airports: {len(le_airport.classes_)} unique\")\n",
    "print(f\"  - Delay Causes: {le_cause.classes_.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef985c9a",
   "metadata": {},
   "source": [
    "### 3.3 Train/Val/Test Split & Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c13e2ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Split Sizes:\n",
      "  Train: 120,166 (70.0%)\n",
      "  Val:   25,750 (15.0%)\n",
      "  Test:  25,750 (15.0%)\n",
      "\n",
      "‚úÖ Normalization complete!\n"
     ]
    }
   ],
   "source": [
    "# Select features for modeling\n",
    "feature_cols = [\n",
    "    'year', 'month', 'month_sin', 'month_cos',\n",
    "    'carrier_encoded', 'airport_encoded',\n",
    "    'arr_flights', 'delay_rate', 'cancel_rate',\n",
    "    'carrier_delay_history', 'airport_delay_history',\n",
    "    'carrier_ct', 'weather_ct', 'nas_ct', 'security_ct', 'late_aircraft_ct'\n",
    "]\n",
    "\n",
    "target_binary = 'is_delayed'\n",
    "target_multiclass = 'cause_encoded'\n",
    "target_regression = 'arr_delay'\n",
    "\n",
    "# Create feature matrix\n",
    "X = df_delay_model[feature_cols].copy()\n",
    "y_binary = df_delay_model[target_binary].copy()\n",
    "y_multiclass = df_delay_model[target_multiclass].copy()\n",
    "y_regression = df_delay_model[target_regression].fillna(0).copy()\n",
    "\n",
    "# Time-based split (use last 20% as test, middle 10% as validation)\n",
    "n = len(X)\n",
    "train_end = int(0.7 * n)\n",
    "val_end = int(0.85 * n)\n",
    "\n",
    "X_train = X.iloc[:train_end]\n",
    "X_val = X.iloc[train_end:val_end]\n",
    "X_test = X.iloc[val_end:]\n",
    "\n",
    "y_binary_train = y_binary.iloc[:train_end]\n",
    "y_binary_val = y_binary.iloc[train_end:val_end]\n",
    "y_binary_test = y_binary.iloc[val_end:]\n",
    "\n",
    "y_multi_train = y_multiclass.iloc[:train_end]\n",
    "y_multi_val = y_multiclass.iloc[train_end:val_end]\n",
    "y_multi_test = y_multiclass.iloc[val_end:]\n",
    "\n",
    "y_reg_train = y_regression.iloc[:train_end]\n",
    "y_reg_val = y_regression.iloc[train_end:val_end]\n",
    "y_reg_test = y_regression.iloc[val_end:]\n",
    "\n",
    "print(f\"\\nüìä Split Sizes:\")\n",
    "print(f\"  Train: {len(X_train):,} ({len(X_train)/n*100:.1f}%)\")\n",
    "print(f\"  Val:   {len(X_val):,} ({len(X_val)/n*100:.1f}%)\")\n",
    "print(f\"  Test:  {len(X_test):,} ({len(X_test)/n*100:.1f}%)\")\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Save scaler\n",
    "with open(OUTPUT_DIR / 'delay_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(\"\\n‚úÖ Normalization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771acb15",
   "metadata": {},
   "source": [
    "### 3.4 Save Processed Delay Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8288b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Delay prediction dataset saved to processed_data/\n",
      "  - Features: 16\n",
      "  - Binary classes: [0, 1]\n",
      "  - Multi-classes: 6\n"
     ]
    }
   ],
   "source": [
    "# Save as numpy arrays for efficient loading\n",
    "np.save(OUTPUT_DIR / 'delay_X_train.npy', X_train_scaled)\n",
    "np.save(OUTPUT_DIR / 'delay_X_val.npy', X_val_scaled)\n",
    "np.save(OUTPUT_DIR / 'delay_X_test.npy', X_test_scaled)\n",
    "\n",
    "np.save(OUTPUT_DIR / 'delay_y_binary_train.npy', y_binary_train.values)\n",
    "np.save(OUTPUT_DIR / 'delay_y_binary_val.npy', y_binary_val.values)\n",
    "np.save(OUTPUT_DIR / 'delay_y_binary_test.npy', y_binary_test.values)\n",
    "\n",
    "np.save(OUTPUT_DIR / 'delay_y_multi_train.npy', y_multi_train.values)\n",
    "np.save(OUTPUT_DIR / 'delay_y_multi_val.npy', y_multi_val.values)\n",
    "np.save(OUTPUT_DIR / 'delay_y_multi_test.npy', y_multi_test.values)\n",
    "\n",
    "np.save(OUTPUT_DIR / 'delay_y_reg_train.npy', y_reg_train.values)\n",
    "np.save(OUTPUT_DIR / 'delay_y_reg_val.npy', y_reg_val.values)\n",
    "np.save(OUTPUT_DIR / 'delay_y_reg_test.npy', y_reg_test.values)\n",
    "\n",
    "# Save feature names\n",
    "with open(OUTPUT_DIR / 'delay_feature_names.pkl', 'wb') as f:\n",
    "    pickle.dump(feature_cols, f)\n",
    "\n",
    "print(\"\\n‚úÖ Delay prediction dataset saved to processed_data/\")\n",
    "print(f\"  - Features: {len(feature_cols)}\")\n",
    "print(f\"  - Binary classes: {np.unique(y_binary_train).tolist()}\")\n",
    "print(f\"  - Multi-classes: {len(le_cause.classes_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2bff6c",
   "metadata": {},
   "source": [
    "## 4. Project 2: Price Prediction Dataset\n",
    "\n",
    "### 4.1 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37c8e56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PRICE PREDICTION - FEATURE ENGINEERING\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Created 24 features\n",
      "‚úÖ Price range: ‚Çπ1,105 - ‚Çπ123,071\n",
      "‚úÖ Routes: 30 unique\n",
      "\n",
      "‚úÖ Created 24 features\n",
      "‚úÖ Price range: ‚Çπ1,105 - ‚Çπ123,071\n",
      "‚úÖ Routes: 30 unique\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PRICE PREDICTION - FEATURE ENGINEERING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df_price_model = df_pricing.copy()\n",
    "\n",
    "# 1. Extract flight number\n",
    "df_price_model['flight_number'] = df_price_model['flight'].str.extract(r'(\\d+)').astype(float)\n",
    "\n",
    "# 2. Create route feature\n",
    "df_price_model['route'] = df_price_model['source_city'] + '_' + df_price_model['destination_city']\n",
    "\n",
    "# 3. Temporal encoding for departure time\n",
    "time_mapping = {\n",
    "    'Early_Morning': 0,\n",
    "    'Morning': 1,\n",
    "    'Afternoon': 2,\n",
    "    'Evening': 3,\n",
    "    'Night': 4,\n",
    "    'Late_Night': 5\n",
    "}\n",
    "df_price_model['departure_time_encoded'] = df_price_model['departure_time'].map(time_mapping)\n",
    "df_price_model['arrival_time_encoded'] = df_price_model['arrival_time'].map(time_mapping)\n",
    "\n",
    "# 4. Cyclic encoding for departure time\n",
    "df_price_model['departure_sin'] = np.sin(2 * np.pi * df_price_model['departure_time_encoded'] / 6)\n",
    "df_price_model['departure_cos'] = np.cos(2 * np.pi * df_price_model['departure_time_encoded'] / 6)\n",
    "\n",
    "# 5. Price per hour of duration\n",
    "df_price_model['price_per_hour'] = df_price_model['price'] / df_price_model['duration']\n",
    "\n",
    "# 6. Booking urgency (inverse of days_left)\n",
    "df_price_model['urgency'] = 1 / (df_price_model['days_left'] + 1)\n",
    "\n",
    "# 7. Stops encoding\n",
    "stops_mapping = {'zero': 0, 'one': 1, 'two_or_more': 2}\n",
    "df_price_model['stops_encoded'] = df_price_model['stops'].map(stops_mapping)\n",
    "\n",
    "# 8. Class encoding\n",
    "df_price_model['class_encoded'] = (df_price_model['class'] == 'Business').astype(int)\n",
    "\n",
    "# 9. Average price by airline\n",
    "airline_avg_price = df_price_model.groupby('airline')['price'].mean()\n",
    "df_price_model['airline_avg_price'] = df_price_model['airline'].map(airline_avg_price)\n",
    "\n",
    "# 10. Average price by route\n",
    "route_avg_price = df_price_model.groupby('route')['price'].mean()\n",
    "df_price_model['route_avg_price'] = df_price_model['route'].map(route_avg_price)\n",
    "\n",
    "print(f\"\\n‚úÖ Created {df_price_model.shape[1]} features\")\n",
    "print(f\"‚úÖ Price range: ‚Çπ{df_price_model['price'].min():,.0f} - ‚Çπ{df_price_model['price'].max():,.0f}\")\n",
    "print(f\"‚úÖ Routes: {df_price_model['route'].nunique()} unique\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3f2ec2",
   "metadata": {},
   "source": [
    "### 4.2 Encoding & Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61c5360b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Encoding complete!\n",
      "  - Airlines: 6\n",
      "  - Routes: 30\n"
     ]
    }
   ],
   "source": [
    "# Encode categorical variables\n",
    "le_airline_price = LabelEncoder()\n",
    "le_source = LabelEncoder()\n",
    "le_dest = LabelEncoder()\n",
    "le_route = LabelEncoder()\n",
    "\n",
    "df_price_model['airline_encoded'] = le_airline_price.fit_transform(df_price_model['airline'])\n",
    "df_price_model['source_encoded'] = le_source.fit_transform(df_price_model['source_city'])\n",
    "df_price_model['dest_encoded'] = le_dest.fit_transform(df_price_model['destination_city'])\n",
    "df_price_model['route_encoded'] = le_route.fit_transform(df_price_model['route'])\n",
    "\n",
    "# Save encoders\n",
    "with open(OUTPUT_DIR / 'price_encoders.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'airline': le_airline_price,\n",
    "        'source': le_source,\n",
    "        'destination': le_dest,\n",
    "        'route': le_route\n",
    "    }, f)\n",
    "\n",
    "print(\"‚úÖ Encoding complete!\")\n",
    "print(f\"  - Airlines: {len(le_airline_price.classes_)}\")\n",
    "print(f\"  - Routes: {len(le_route.classes_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7354f8",
   "metadata": {},
   "source": [
    "### 4.3 Train/Val/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a492517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Split Sizes:\n",
      "  Train: 210,107 (70%)\n",
      "  Val:   45,023 (15%)\n",
      "  Test:  45,023 (15%)\n",
      "\n",
      "‚úÖ Normalization complete!\n"
     ]
    }
   ],
   "source": [
    "# Select features\n",
    "price_features = [\n",
    "    'airline_encoded', 'source_encoded', 'dest_encoded', 'route_encoded',\n",
    "    'departure_time_encoded', 'arrival_time_encoded',\n",
    "    'departure_sin', 'departure_cos',\n",
    "    'stops_encoded', 'class_encoded',\n",
    "    'duration', 'days_left', 'urgency',\n",
    "    'airline_avg_price', 'route_avg_price'\n",
    "]\n",
    "\n",
    "X_price = df_price_model[price_features].copy()\n",
    "y_price = df_price_model['price'].copy()\n",
    "\n",
    "# Random split (since no temporal component in this dataset)\n",
    "X_price_train, X_price_temp, y_price_train, y_price_temp = train_test_split(\n",
    "    X_price, y_price, test_size=0.3, random_state=42\n",
    ")\n",
    "X_price_val, X_price_test, y_price_val, y_price_test = train_test_split(\n",
    "    X_price_temp, y_price_temp, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Split Sizes:\")\n",
    "print(f\"  Train: {len(X_price_train):,} (70%)\")\n",
    "print(f\"  Val:   {len(X_price_val):,} (15%)\")\n",
    "print(f\"  Test:  {len(X_price_test):,} (15%)\")\n",
    "\n",
    "# Normalize features\n",
    "scaler_price = StandardScaler()\n",
    "X_price_train_scaled = scaler_price.fit_transform(X_price_train)\n",
    "X_price_val_scaled = scaler_price.transform(X_price_val)\n",
    "X_price_test_scaled = scaler_price.transform(X_price_test)\n",
    "\n",
    "# Log-transform target (price is right-skewed)\n",
    "y_price_train_log = np.log1p(y_price_train)\n",
    "y_price_val_log = np.log1p(y_price_val)\n",
    "y_price_test_log = np.log1p(y_price_test)\n",
    "\n",
    "# Save scaler\n",
    "with open(OUTPUT_DIR / 'price_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_price, f)\n",
    "\n",
    "print(\"\\n‚úÖ Normalization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c21947",
   "metadata": {},
   "source": [
    "### 4.4 Save Processed Price Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4887c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Price prediction dataset saved!\n",
      "  - Features: 15\n",
      "  - Price statistics (train):\n",
      "    Mean: ‚Çπ20,896\n",
      "    Std:  ‚Çπ22,703\n"
     ]
    }
   ],
   "source": [
    "# Save arrays\n",
    "np.save(OUTPUT_DIR / 'price_X_train.npy', X_price_train_scaled)\n",
    "np.save(OUTPUT_DIR / 'price_X_val.npy', X_price_val_scaled)\n",
    "np.save(OUTPUT_DIR / 'price_X_test.npy', X_price_test_scaled)\n",
    "\n",
    "np.save(OUTPUT_DIR / 'price_y_train.npy', y_price_train.values)\n",
    "np.save(OUTPUT_DIR / 'price_y_val.npy', y_price_val.values)\n",
    "np.save(OUTPUT_DIR / 'price_y_test.npy', y_price_test.values)\n",
    "\n",
    "np.save(OUTPUT_DIR / 'price_y_train_log.npy', y_price_train_log.values)\n",
    "np.save(OUTPUT_DIR / 'price_y_val_log.npy', y_price_val_log.values)\n",
    "np.save(OUTPUT_DIR / 'price_y_test_log.npy', y_price_test_log.values)\n",
    "\n",
    "# Save feature names\n",
    "with open(OUTPUT_DIR / 'price_feature_names.pkl', 'wb') as f:\n",
    "    pickle.dump(price_features, f)\n",
    "\n",
    "print(\"\\n‚úÖ Price prediction dataset saved!\")\n",
    "print(f\"  - Features: {len(price_features)}\")\n",
    "print(f\"  - Price statistics (train):\")\n",
    "print(f\"    Mean: ‚Çπ{y_price_train.mean():,.0f}\")\n",
    "print(f\"    Std:  ‚Çπ{y_price_train.std():,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322c2be8",
   "metadata": {},
   "source": [
    "## 5. Project 3: Passenger Forecasting Dataset\n",
    "\n",
    "### 5.1 Time Series Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfbcca09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PASSENGER FORECASTING - TIME SERIES PREPARATION\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Time series features created\n",
      "  - Countries: 89\n",
      "  - Date range: 2010-01-01 00:00:00 to 2017-12-01 00:00:00\n",
      "  - Total records: 6,594\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PASSENGER FORECASTING - TIME SERIES PREPARATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df_passenger_model = df_passengers.copy()\n",
    "\n",
    "# Use Total_OS column (most complete)\n",
    "df_passenger_model = df_passenger_model[['ISO3', 'Year', 'Month', 'Total_OS']].copy()\n",
    "df_passenger_model = df_passenger_model.dropna(subset=['Total_OS'])\n",
    "\n",
    "# Create datetime\n",
    "df_passenger_model['date'] = pd.to_datetime(\n",
    "    df_passenger_model[['Year', 'Month']].assign(day=1)\n",
    ")\n",
    "\n",
    "# Sort by country and date\n",
    "df_passenger_model = df_passenger_model.sort_values(['ISO3', 'date'])\n",
    "\n",
    "# Create temporal features\n",
    "df_passenger_model['month_sin'] = np.sin(2 * np.pi * df_passenger_model['Month'] / 12)\n",
    "df_passenger_model['month_cos'] = np.cos(2 * np.pi * df_passenger_model['Month'] / 12)\n",
    "df_passenger_model['year_normalized'] = (df_passenger_model['Year'] - df_passenger_model['Year'].min()) / \\\n",
    "                                         (df_passenger_model['Year'].max() - df_passenger_model['Year'].min())\n",
    "\n",
    "# Create lag features (previous 1, 3, 6, 12 months)\n",
    "for lag in [1, 3, 6, 12]:\n",
    "    df_passenger_model[f'lag_{lag}'] = df_passenger_model.groupby('ISO3')['Total_OS'].shift(lag)\n",
    "\n",
    "# Rolling statistics (3-month and 6-month windows)\n",
    "for window in [3, 6]:\n",
    "    df_passenger_model[f'rolling_mean_{window}'] = df_passenger_model.groupby('ISO3')['Total_OS'].transform(\n",
    "        lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "    )\n",
    "    df_passenger_model[f'rolling_std_{window}'] = df_passenger_model.groupby('ISO3')['Total_OS'].transform(\n",
    "        lambda x: x.rolling(window=window, min_periods=1).std()\n",
    "    )\n",
    "\n",
    "# Fill NaN in lag/rolling features with forward fill\n",
    "df_passenger_model = df_passenger_model.fillna(method='ffill').fillna(0)\n",
    "\n",
    "print(f\"\\n‚úÖ Time series features created\")\n",
    "print(f\"  - Countries: {df_passenger_model['ISO3'].nunique()}\")\n",
    "print(f\"  - Date range: {df_passenger_model['date'].min()} to {df_passenger_model['date'].max()}\")\n",
    "print(f\"  - Total records: {len(df_passenger_model):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33deb97e",
   "metadata": {},
   "source": [
    "### 5.2 Encode & Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50fe4928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Split Sizes:\n",
      "  Train: 4,615\n",
      "  Val:   989\n",
      "  Test:  990\n",
      "\n",
      "‚úÖ Normalization complete!\n"
     ]
    }
   ],
   "source": [
    "# Encode country\n",
    "le_country = LabelEncoder()\n",
    "df_passenger_model['country_encoded'] = le_country.fit_transform(df_passenger_model['ISO3'])\n",
    "\n",
    "# Save encoder\n",
    "with open(OUTPUT_DIR / 'passenger_encoders.pkl', 'wb') as f:\n",
    "    pickle.dump({'country': le_country}, f)\n",
    "\n",
    "# Select features\n",
    "passenger_features = [\n",
    "    'country_encoded', 'Year', 'Month',\n",
    "    'month_sin', 'month_cos', 'year_normalized',\n",
    "    'lag_1', 'lag_3', 'lag_6', 'lag_12',\n",
    "    'rolling_mean_3', 'rolling_std_3',\n",
    "    'rolling_mean_6', 'rolling_std_6'\n",
    "]\n",
    "\n",
    "X_passenger = df_passenger_model[passenger_features].copy()\n",
    "y_passenger = df_passenger_model['Total_OS'].copy()\n",
    "\n",
    "# Time-based split (70/15/15)\n",
    "n = len(X_passenger)\n",
    "train_end = int(0.7 * n)\n",
    "val_end = int(0.85 * n)\n",
    "\n",
    "X_pass_train = X_passenger.iloc[:train_end]\n",
    "X_pass_val = X_passenger.iloc[train_end:val_end]\n",
    "X_pass_test = X_passenger.iloc[val_end:]\n",
    "\n",
    "y_pass_train = y_passenger.iloc[:train_end]\n",
    "y_pass_val = y_passenger.iloc[train_end:val_end]\n",
    "y_pass_test = y_passenger.iloc[val_end:]\n",
    "\n",
    "print(f\"\\nüìä Split Sizes:\")\n",
    "print(f\"  Train: {len(X_pass_train):,}\")\n",
    "print(f\"  Val:   {len(X_pass_val):,}\")\n",
    "print(f\"  Test:  {len(X_pass_test):,}\")\n",
    "\n",
    "# Normalize\n",
    "scaler_passenger = StandardScaler()\n",
    "X_pass_train_scaled = scaler_passenger.fit_transform(X_pass_train)\n",
    "X_pass_val_scaled = scaler_passenger.transform(X_pass_val)\n",
    "X_pass_test_scaled = scaler_passenger.transform(X_pass_test)\n",
    "\n",
    "# Save scaler\n",
    "with open(OUTPUT_DIR / 'passenger_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_passenger, f)\n",
    "\n",
    "print(\"\\n‚úÖ Normalization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97af3353",
   "metadata": {},
   "source": [
    "### 5.3 Save Passenger Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfddc0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Passenger forecasting dataset saved!\n",
      "  - Features: 14\n",
      "  - Countries: 89\n"
     ]
    }
   ],
   "source": [
    "# Save arrays\n",
    "np.save(OUTPUT_DIR / 'passenger_X_train.npy', X_pass_train_scaled)\n",
    "np.save(OUTPUT_DIR / 'passenger_X_val.npy', X_pass_val_scaled)\n",
    "np.save(OUTPUT_DIR / 'passenger_X_test.npy', X_pass_test_scaled)\n",
    "\n",
    "np.save(OUTPUT_DIR / 'passenger_y_train.npy', y_pass_train.values)\n",
    "np.save(OUTPUT_DIR / 'passenger_y_val.npy', y_pass_val.values)\n",
    "np.save(OUTPUT_DIR / 'passenger_y_test.npy', y_pass_test.values)\n",
    "\n",
    "# Save feature names\n",
    "with open(OUTPUT_DIR / 'passenger_feature_names.pkl', 'wb') as f:\n",
    "    pickle.dump(passenger_features, f)\n",
    "\n",
    "print(\"\\n‚úÖ Passenger forecasting dataset saved!\")\n",
    "print(f\"  - Features: {len(passenger_features)}\")\n",
    "print(f\"  - Countries: {len(le_country.classes_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881cae37",
   "metadata": {},
   "source": [
    "## 6. Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2dd3c902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "‚úÖ PREPROCESSING COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "üì¶ Saved Datasets:\n",
      "\n",
      "1. DELAY PREDICTION\n",
      "   - Binary classification (delayed/on-time)\n",
      "   - Multi-class classification (delay cause: 6 classes)\n",
      "   - Regression (delay minutes)\n",
      "   - Features: 16\n",
      "   - Train samples: 120,166\n",
      "\n",
      "2. PRICE PREDICTION\n",
      "   - Regression task (predict ticket price)\n",
      "   - Features: 15\n",
      "   - Train samples: 210,107\n",
      "\n",
      "3. PASSENGER FORECASTING\n",
      "   - Time series regression\n",
      "   - Features: 14 (includes lags & rolling stats)\n",
      "   - Train samples: 4,615\n",
      "\n",
      "üöÄ Next Steps:\n",
      "   ‚Üí Notebook 03: Build delay prediction models (feedforward NN)\n",
      "   ‚Üí Notebook 04: Build price prediction models (DNN with embeddings)\n",
      "   ‚Üí Notebook 05: Advanced models (TabNet, LSTM, Transformers)\n",
      "\n",
      "üìÅ All processed data saved to: processed_data/\n",
      "   Total files: 36\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ PREPROCESSING COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüì¶ Saved Datasets:\")\n",
    "print(\"\\n1. DELAY PREDICTION\")\n",
    "print(f\"   - Binary classification (delayed/on-time)\")\n",
    "print(f\"   - Multi-class classification (delay cause: {len(le_cause.classes_)} classes)\")\n",
    "print(f\"   - Regression (delay minutes)\")\n",
    "print(f\"   - Features: {len(feature_cols)}\")\n",
    "print(f\"   - Train samples: {len(X_train):,}\")\n",
    "\n",
    "print(\"\\n2. PRICE PREDICTION\")\n",
    "print(f\"   - Regression task (predict ticket price)\")\n",
    "print(f\"   - Features: {len(price_features)}\")\n",
    "print(f\"   - Train samples: {len(X_price_train):,}\")\n",
    "\n",
    "print(\"\\n3. PASSENGER FORECASTING\")\n",
    "print(f\"   - Time series regression\")\n",
    "print(f\"   - Features: {len(passenger_features)} (includes lags & rolling stats)\")\n",
    "print(f\"   - Train samples: {len(X_pass_train):,}\")\n",
    "\n",
    "print(\"\\nüöÄ Next Steps:\")\n",
    "print(\"   ‚Üí Notebook 03: Build delay prediction models (feedforward NN)\")\n",
    "print(\"   ‚Üí Notebook 04: Build price prediction models (DNN with embeddings)\")\n",
    "print(\"   ‚Üí Notebook 05: Advanced models (TabNet, LSTM, Transformers)\")\n",
    "\n",
    "print(\"\\nüìÅ All processed data saved to: processed_data/\")\n",
    "saved_files = list(OUTPUT_DIR.glob('*.npy')) + list(OUTPUT_DIR.glob('*.pkl'))\n",
    "print(f\"   Total files: {len(saved_files)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.9.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
